# Deep-learning-university-course
## Lab 1
Just a bunch of different activation functions, their derivatives and loss function with their plots, the point of this lab was to take a closer look into different activations/losses behaviour
## Lab 2
In this lab we had a Parameter class code, which was inspired by Andrej Karpathy "micrograd" project, our task was to implement backward(using topological sort) and our activation functions from lab1, then we implemented the gradient descent algorithm and tested it by optimizing some linear function
## Lab 3
In this lab we had to create our own MLP class using torch library, we also needed to implement data loading, transforming and training loop
