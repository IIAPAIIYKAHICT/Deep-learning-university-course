{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10e0756-975b-4e33-9ff5-ccaa92435596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2090da17-3cbe-4551-9104-73e02ae1f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, value: float, name: str, _children=()) -> None:\n",
    "        self._value = value\n",
    "        self._name = name\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._grad = 0.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Parameter {self._name} = {self._value}; dL/d[{self._name}] = {self._grad}\"\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Parameter) else Parameter(other._value, \"const\", ())\n",
    "        result = Parameter(\n",
    "            self._value * other._value,\n",
    "            f'{self._name} * {other._name}',\n",
    "            _children=(self, other)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self._grad += other._value * result._grad\n",
    "            other._grad += self._value * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        result = Parameter(\n",
    "            self._value**power, \n",
    "            f\"{self._name}**{power}\",\n",
    "            _children=(self,)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self._grad += power * (self._value ** (power - 1)) * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Parameter) else Parameter(other._value, \"const\", ())\n",
    "        result = Parameter(\n",
    "            self._value + other._value,\n",
    "            f'[{self._name} + {other._name}]',\n",
    "            _children=(self, other)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self._grad += 1.0 * result._grad\n",
    "            other._grad += 1.0 * result._grad\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __neg__(self): \n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): \n",
    "        return self + (-other)\n",
    "\n",
    "    def sigmoid(self):\n",
    "        val = 1.0 / (1.0 + math.exp(-self._value))\n",
    "\n",
    "        result = Parameter(\n",
    "            val,\n",
    "            f\"σ({self._name})\",\n",
    "            _children=(self,)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self._grad += result._grad * val * (1 - val)\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "    def backward(self):\n",
    "        topo_sort = []\n",
    "        visited_nodes = set()\n",
    "        def sort_topo(node):\n",
    "            if node not in visited_nodes:\n",
    "                visited_nodes.add(node)\n",
    "                for child in node._prev:\n",
    "                    sort_topo(child)\n",
    "                topo_sort.append(node)\n",
    "        sort_topo(self)\n",
    "        \n",
    "        self._grad = 1.0\n",
    "        for node in reversed(topo_sort):\n",
    "            node._backward()\n",
    "\n",
    "    def softplus(self):\n",
    "        val = np.log(1 + np.exp(self._value))\n",
    "        result = Parameter(\n",
    "            val,\n",
    "            f\"softplus({self._name})\",\n",
    "            _children=(self,)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            self._grad += result._grad * (1.0 / (1.0 + np.exp(-self._value)))\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "    def mish(self):\n",
    "        softplus_result = self.softplus()\n",
    "        tanh_val = np.tanh(softplus_result._value)\n",
    "        val = self._value * tanh_val\n",
    "\n",
    "        result = Parameter(\n",
    "            val,\n",
    "            f\"mish({self._name})\",\n",
    "            _children=(self, softplus_result)\n",
    "        )\n",
    "\n",
    "        def _backward():\n",
    "            sigmoid_val = 1.0 / (1.0 + np.exp(-softplus_result._value))\n",
    "            sech_sqr = 1 / np.cosh(softplus_result._value)**2\n",
    "            mish_grad = tanh_val + self._value * sech_sqr * sigmoid_val * (1 - tanh_val**2)\n",
    "            self._grad += result._grad * mish_grad\n",
    "\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def gd(learning_rate: float, *parameters: Parameter) -> None:\n",
    "    for parameter in parameters:\n",
    "        parameter._value -= learning_rate * parameter._grad\n",
    "        parameter._grad = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dc1c2",
   "metadata": {},
   "source": [
    "## Backward method test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fd73e5-da08-4969-9038-935b2d17f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter [a * b + c] * d = 55.0; dL/d[[a * b + c] * d] = 1.0\n",
      "Parameter [a * b + c] = 11.0; dL/d[[a * b + c]] = 5.0\n",
      "Parameter a * b = 6.0; dL/d[a * b] = 5.0\n",
      "Parameter d = 5.0; dL/d[d] = 11.0\n",
      "Parameter c = 5.0; dL/d[c] = 5.0\n",
      "Parameter b = 2.0; dL/d[b] = 15.0\n",
      "Parameter a = 3.0; dL/d[a] = 10.0\n"
     ]
    }
   ],
   "source": [
    "a = Parameter(3.0, 'a')\n",
    "b = Parameter(2.0, 'b')\n",
    "c = Parameter(5.0, 'c')\n",
    "d = Parameter(5.0, 'd')\n",
    "u = a * b\n",
    "v = u + c\n",
    "L = v * d\n",
    "L.backward()\n",
    "print(L)\n",
    "print(v)\n",
    "print(u)\n",
    "print(d)\n",
    "print(c)\n",
    "print(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f49efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0001837248450104483\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x1 = Parameter(3.0, 'x1')\n",
    "x2 = Parameter(4.0, 'x2')\n",
    "\n",
    "w1 = Parameter(1.0, 'w1')\n",
    "w2 = Parameter(2.0, 'w2')\n",
    "\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "xw = x1w1 + x2w2\n",
    "out = xw.mish()\n",
    "out.backward()\n",
    "print(out._grad)\n",
    "print(xw._grad)\n",
    "print(x2w2._grad)\n",
    "print(x1w1._grad)\n",
    "print(x1._grad)\n",
    "print(w1._grad)\n",
    "print(x2._grad)\n",
    "print(w2._grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09306",
   "metadata": {},
   "source": [
    "## Gradient descent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6083453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epochs :0.08921431830345163\n",
      "loss after 1 epochs :0.08921431830345163\n",
      "loss after 2 epochs :0.08861794298322671\n",
      "loss after 3 epochs :0.08743119234277691\n",
      "loss after 4 epochs :0.0856660663821022\n",
      "loss after 5 epochs :0.08334056510120258\n",
      "loss after 6 epochs :0.08047868850007805\n",
      "loss after 7 epochs :0.07711043657872863\n",
      "loss after 8 epochs :0.0732718093371543\n",
      "loss after 9 epochs :0.06900480677535506\n",
      "loss after 10 epochs :0.06435742889333093\n",
      "loss after 11 epochs :0.05938367569108188\n",
      "loss after 12 epochs :0.05414354716860799\n",
      "loss after 13 epochs :0.048703043325909134\n",
      "loss after 14 epochs :0.04313416416298538\n",
      "loss after 15 epochs :0.03751490967983672\n",
      "loss after 16 epochs :0.03192927987646316\n",
      "loss after 17 epochs :0.02646727475286469\n",
      "loss after 18 epochs :0.021224894309041318\n",
      "loss after 19 epochs :0.016304138544993043\n",
      "loss after 20 epochs :0.011813007460719867\n",
      "loss after 21 epochs :0.007865501056221798\n",
      "loss after 22 epochs :0.004581619331498822\n",
      "loss after 23 epochs :0.0020873622865509333\n",
      "loss after 24 epochs :0.0005147299213781425\n"
     ]
    }
   ],
   "source": [
    "W = Parameter(0.5, 'W') \n",
    "b = Parameter(0.1, 'b') \n",
    "x = Parameter(0.8, 'x') \n",
    "learning_rate = 0.001\n",
    "n_epochs = 25\n",
    "target = 0.4\n",
    "for n in range(n_epochs):\n",
    "    y = (W * x).sigmoid() + b  \n",
    "    gd(learning_rate, W, b)\n",
    "    loss = (y._value - target) ** 2\n",
    "    y.backward()\n",
    "    print(f\"loss after {n} epochs :{loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4a9f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter [σ(W * x) + b] = 0.398687660112452; dL/d[[σ(W * x) + b]] = 0.0\n"
     ]
    }
   ],
   "source": [
    "print((W * x).sigmoid() + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
