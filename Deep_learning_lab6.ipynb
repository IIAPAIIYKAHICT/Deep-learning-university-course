{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b569e341-8b89-4202-917a-c7fe7af8eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b159343e-86c0-4f9e-b3ec-398fae984814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries: (B, N, D)\n",
    "        # keys: (B, M, D)\n",
    "        # values: (B, M, V)\n",
    "\n",
    "        assert queries.shape[0] == keys.shape[0]\n",
    "        assert queries.shape[0] == values.shape[0]\n",
    "        assert queries.shape[-1] == keys.shape[-1]\n",
    "        assert keys.shape[1] == values.shape[1]\n",
    "\n",
    "        d = keys.shape[-1]\n",
    "\n",
    "        # scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        scores = queries @ keys.transpose(1, 2) / math.sqrt(d)\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        return attention_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fc4258a-e1b5-439e-86ad-4b4678ea7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.zeros((2, 1, 2))\n",
    "keys = torch.zeros((2, 10, 2))\n",
    "values = torch.zeros((2, 10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d37c5880-8f9e-4cd8-b141-b0844b34782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = DotProductAttention()\n",
    "att(queries, keys, values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c7e6811-eb84-4792-9a76-6eac385f3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hiddens = num_hiddens\n",
    "\n",
    "        self.attention = DotProductAttention()\n",
    "\n",
    "        self.W_q = nn.LazyLinear(num_hiddens)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens)\n",
    "\n",
    "        self.W_o = nn.LazyLinear(num_hiddens)\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "        # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "    \n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # qkv: (B, N_qkv, num_hiddens)\n",
    "\n",
    "        q = self.transpose_qkv(self.W_q(queries))\n",
    "        k = self.transpose_qkv(self.W_k(keys))\n",
    "        v = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        output = self.transpose_output(self.attention(q, k, v))\n",
    "\n",
    "        return self.W_o(output)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e6d7254-e994-45a1-b416-106935a67dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adamt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 100])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 100\n",
    "num_heads = 5\n",
    "\n",
    "mha = MultiHeadAttention(num_hiddens, num_heads)\n",
    "\n",
    "X = torch.ones((16, 5, 5))\n",
    "mha(X, X, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14ba6ad2-3ae7-41d9-8924-52abefe45fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, max_tokens=1000):\n",
    "        super().__init__()\n",
    "        self.P = torch.zeros((1, max_tokens, num_hiddens))\n",
    "        X = torch.arange(max_tokens, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30959d50-65e6-4b14-88ed-b7b24e638352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 32]) torch.Size([1, 60, 32])\n"
     ]
    }
   ],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pe = PositionalEncoding(encoding_dim)\n",
    "X = torch.zeros((1, num_steps, encoding_dim))\n",
    "XPE = pe(X)\n",
    "\n",
    "print(X.shape, XPE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f040803b-adec-4885-aff6-d5a6916138bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n",
       "           1.7783e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n",
       "           3.5566e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 4.3616e-01,  8.9987e-01,  5.9521e-01,  ...,  9.9984e-01,\n",
       "           1.0136e-02,  9.9995e-01],\n",
       "         [ 9.9287e-01,  1.1918e-01,  9.3199e-01,  ...,  9.9983e-01,\n",
       "           1.0314e-02,  9.9995e-01],\n",
       "         [ 6.3674e-01, -7.7108e-01,  9.8174e-01,  ...,  9.9983e-01,\n",
       "           1.0492e-02,  9.9994e-01]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "865563d3-4e02-4cb1-bb74-6df03961941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, ffn_dim, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.LazyLinear(ffn_dim)\n",
    "        self.w2 = nn.LazyLinear(num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.w2(self.relu(self.w1(X)))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, res, X):\n",
    "        return self.ln(X + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03df506f-5916-4a5f-bdf1-3d9ca317f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_dim, num_heads):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads)\n",
    "        self.ffn = FFN(ffn_dim, num_hiddens)\n",
    "        self.addnorm1 = AddNorm(num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26b120e5-1c30-4028-a235-b2c83efd5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_blocks, ffn_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.positional_encoding  = PositionalEncoding(num_hiddens)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(TransformerEncoderBlock(num_hiddens, ffn_dim, num_heads))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            X = block(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "901d6a6a-1c1d-40e9-bfbb-2123479c2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(1000, 300, 6, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7e11fe5-7184-4393-bec9-9e078886f134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 300])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.zeros((2, 100)).long()\n",
    "encoder(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a355a90-4b36-4e8e-9704-9c81a91a92ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3ecd352-cea1-491c-ba8c-1c53b4d4721d",
   "metadata": {},
   "source": [
    "<h2>Your supertask!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4904389-345a-487e-8461-7ff73f04cd52",
   "metadata": {},
   "source": [
    "Using this encoder implementation try to implement ViT. Have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "808a2bf6-6896-4d5a-8e38-e8ad7f5e2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embedding_dim, channels=3):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.flatten_dim = patch_size * patch_size * channels\n",
    "\n",
    "        self.linear_projection = nn.Linear(self.flatten_dim, embedding_dim)\n",
    "    def forward(self, X):\n",
    "        #X shape (B, C, H, W)\n",
    "        X = X.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) #creating patches\n",
    "        X = X.contiguous().view(X.size(0), X.size(1), X.size(2) * X.size(3), -1)\n",
    "        X = X.permute(0, 2, 1, 3).contiguous().view(X.size(0), X.size(2), -1)\n",
    "        return self.linear_projection(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2685a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderVit(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_blocks, ffn_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionalEncoding(num_hiddens)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(TransformerEncoderBlock(num_hiddens, ffn_dim, num_heads))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.positional_encoding(X)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b909072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embedding_dim, num_blocks, ffn_dim, num_heads, channels=3):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, embedding_dim, channels)\n",
    "        self.transformer_encoder = TransformerEncoderVit(embedding_dim, num_blocks, ffn_dim, num_heads)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embedding(X)\n",
    "        X = self.transformer_encoder(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca00e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "img_size = 224  \n",
    "patch_size = 16\n",
    "channels = 3   \n",
    "embedding_dim = 768\n",
    "num_blocks = 12\n",
    "ffn_dim = 2048\n",
    "num_heads = 12\n",
    "\n",
    "vit = ViT(img_size, patch_size, embedding_dim, num_blocks=num_blocks, ffn_dim=ffn_dim, num_heads=num_heads, channels=channels)\n",
    "dummy_image = torch.randn(1, channels, img_size, img_size)\n",
    "output = vit(dummy_image)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff9427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
